{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import os\n",
    "import re\n",
    "import sys\n",
    "from os.path import join\n",
    "from pprint import pprint\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "sys.path.append('../bias_probing')\n",
    "import bias_probing.config as project_config\n",
    "\n",
    "import pickle\n",
    "import matplotlib.pyplot as plt\n",
    "import matplotlib as mpl\n",
    "from IPython.display import display, HTML\n",
    "\n",
    "%matplotlib inline\n",
    "%load_ext autoreload\n",
    "%autoreload 2"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Notebook Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import matplotlib.font_manager as fm\n",
    "# Collect all the font names available to matplotlib\n",
    "font_names = [f.name for f in fm.fontManager.ttflist]\n",
    "print([f for f in font_names if 'serif' in f.lower()])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Notebook settings, do not modify\n",
    "mpl.rcParams['figure.dpi'] = 300\n",
    "# mpl.rcParams['font.family'] = 'Microsoft Sans Serif'\n",
    "plt.rcParams['font.size'] = 30\n",
    "plt.rcParams['axes.linewidth'] = 2\n",
    "plt.rcParams['figure.figsize'] = (4.7747, 3.5)\n",
    "plt.style.use('ggplot')\n",
    "mpl.use('pgf')\n",
    "mpl.rcParams.update({\n",
    "    'pgf.texsystem': \"pdflatex\",\n",
    "    'font.family': 'serif',\n",
    "    'text.usetex': True,\n",
    "    'pgf.rcfonts': False,\n",
    "    'pgf.preamble': [\n",
    "        '\\DeclareUnicodeCharacter{2212}{-}'\n",
    "    ]\n",
    "})\n",
    "\n",
    "pd.set_option('display.max_colwidth', 40)\n",
    "\n",
    "%matplotlib inline\n",
    "# %matplotlib widget\n",
    "%load_ext autoreload\n",
    "%autoreload 2\n",
    "\n",
    "figures_dir = project_config.FIGURES_DIR"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "deletable": false
   },
   "outputs": [],
   "source": [
    "# Group by all models of the same type\n",
    "def results_for_task_all_seeds(results, task_config, probe_type='linear'):\n",
    "    return results[(results['task_config_file'] == task_config) & (results['probe_type'] == probe_type)]\\\n",
    "        [['compression', 'online_cdl', 'eval_accuracy']]\\\n",
    "        .groupby(results['model_name_or_path'].str.replace('_seed_.*', ''))\\\n",
    "        .agg(['std', 'mean', 'count'])\\\n",
    "        .round(3)\\\n",
    "        .sort_values([('online_cdl',  'mean')], ascending=False)\n",
    "\n",
    "#         .drop('name.1', axis=1, errors='ignore')\\\n",
    "#         .drop('output_dir', axis=1, errors='ignore')\\\n",
    "#         .drop('type', axis=1, errors='ignore')\\\n",
    "#         .drop('seed', axis=1, errors='ignore')\\\n",
    "#         .drop('embedding_size', axis=1, errors='ignore')\\\n",
    "\n",
    "def load_experiment_results(output_dir):\n",
    "    p = os.path.join(output_dir, f'results.pkl')\n",
    "    with open(p, 'rb') as f:\n",
    "        exps = pickle.load(f)\n",
    "    return exps\n",
    "\n",
    "\n",
    "def plot_mean_with_std(results, name_pretty_dict=None, replace='_[0-9]+', title=None):\n",
    "    df = pd.DataFrame()\n",
    "    df.index.name = 'name'\n",
    "    fractions = None\n",
    "    for out_dir, name in zip(list(results.output_dir), list(results.name)):\n",
    "        exp = load_experiment_results(out_dir)\n",
    "        if fractions is None:\n",
    "            fractions = exp.fractions\n",
    "        loss_list = pd.Series(list(map(lambda r: r['eval_loss'], exp.report['online_coding_list'])), \\\n",
    "                              name=name)\n",
    "        if df is None:\n",
    "            df = loss_list.to_frame()\n",
    "        else:\n",
    "            df = df.append(loss_list)\n",
    "\n",
    "    df_mean = df.groupby(df.index.str.replace(replace, '')).agg(['mean', 'std'])\n",
    "    fig, ax = plt.subplots() \n",
    "    for name in filter(lambda k: k in name_pretty_dict, df_mean.index):\n",
    "        x = []\n",
    "        y = []\n",
    "        e = []\n",
    "        for col in df_mean.columns:\n",
    "            idx, typ = col\n",
    "            if typ == 'mean':\n",
    "                x.append(idx)\n",
    "                y.append(df_mean.loc[name, col])\n",
    "            elif typ == 'std':\n",
    "                e.append(df_mean.loc[name, col])\n",
    "        plt.errorbar(x, y, e, linestyle='-', marker='^', label=name if name_pretty_dict is None else name_pretty_dict[name])\n",
    "    \n",
    "    ax.set_xticks(x)\n",
    "    ax.set_xticklabels([str(i) for i in fractions[:-1]])\n",
    "    ax.set_xlabel('% of training data')\n",
    "    ax.set_ylabel('Test error')\n",
    "    # plt.title(title)\n",
    "    L = ax.legend()\n",
    "    plt.setp(L.texts, family='sans-serif')\n",
    "    plt.legend(borderpad=1, loc='lower center', bbox_to_anchor=(0.5, -0.3), ncol=len(df_mean))\n",
    "    if title is not None:\n",
    "        plt.savefig(join(figures_dir, title.replace(' ', '_') + '.png'), bbox_inches='tight', dpi=300)\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "def show_results(df: pd.DataFrame, tasks):\n",
    "    for task_config in tasks:\n",
    "        print(f'{task_config}')\n",
    "        display(results_for_task_all_seeds(df, task_config, probe_type='linear'))\n",
    "        plot_mean_with_std(df[df['task_config_file'] == task_config], name_pretty_dict)\n",
    "\n",
    "    \n",
    "    CSS = \"\"\"\n",
    "    .output {\n",
    "    }\n",
    "    \"\"\"\n",
    "\n",
    "    return HTML('<style>{}</style>'.format(CSS))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "deletable": false,
    "editable": false
   },
   "source": [
    "## Experiment Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "tasks_hypo = [\n",
    "    \"mnli_neg_words_config.json\",\n",
    "    \"mnli_neg_words_hypo_only_config.json\",\n",
    "    \"snli_neg_words_config.json\",\n",
    "    \"snli_neg_words_hypo_only_config.json\",\n",
    "    \"fever_neg_words_config.json\",\n",
    "    \"fever_neg_words_hypo_only_config.json\",\n",
    "]\n",
    "\n",
    "tasks_hans = [\n",
    "    \"mnli_hclass_lex_config.json\",\n",
    "    \"mnli_hclass_sub_config.json\",\n",
    "    \"snli_hclass_lex_config.json\",\n",
    "    \"snli_hclass_sub_config.json\",\n",
    "]\n",
    "\n",
    "name_pretty_dict = {\n",
    "    'baseline': 'Base',\n",
    "    'dfl': 'DFL',\n",
    "    'pretrained': 'Pretrained',\n",
    "    'random': 'Random',\n",
    "    'bert_confreg': 'ConfReg',\n",
    "    'bert_implicit': 'Implicit',\n",
    "    'bert_tiny': 'TinyBERT'\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results.groupby('task_config_file').count().mean(axis=1).rename('Number of experiments').to_frame()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hans = pd.read_csv(join(project_config.TEMP_DIR, 'results_hans.csv'))\n",
    "# len(results_hans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show_results(results_hans, tasks_hans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hypo = pd.read_csv(join(project_config.TEMP_DIR, 'results_hypo.csv'))\n",
    "# len(results_hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# show_results(results_hypo, tasks_hypo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hypo_bigrams = pd.read_csv(join(project_config.TEMP_DIR, 'results_hypo_bigrams.csv'))\n",
    "# display(results_for_task_all_seeds(results_hypo_bigrams, 'mnli_neg_bigrams_hypo_only_config.json', probe_type='linear'))\n",
    "# plot_mean_with_std(results_hypo_bigrams, name_pretty_dict=name_pretty_dict)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hypo_dfl_gamma = pd.read_csv(join(project_config.TEMP_DIR, 'results_hypo_dfl_gamma.csv'))\n",
    "# results_hypo_dfl_gamma[:5]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tasks = tasks_mnli\n",
    "# for task_config in tasks:\n",
    "#     print(f'{task_config}')\n",
    "#     plot_dfl_gamma_experiment_results(task_config, results_hypo_dfl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hans_dfl_gamma = pd.read_csv(join(project_config.TEMP_DIR, 'results_hans_dfl_gamma_new.csv'))\n",
    "# len(results_hans_dfl_gamma)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "# tasks = tasks_hans\n",
    "# for task_config in tasks:\n",
    "#     print(f'{task_config}')\n",
    "#     plot_dfl_gamma_experiment_results(task_config, results_hans_dfl_gamma)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## LMI Analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from experiments.calculate_lmi import *\n",
    "# task = jiant_create_task_from_config_path(os.path.join(project_config.DATA_DIR, 'datasets/configs/fever_nli_config.json'))\n",
    "# dataset = task.get_train_examples()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# tokenizer = BertTokenizer.from_pretrained('bert-base-uncased')\n",
    "\n",
    "# def tokenize(sent):\n",
    "#     return tokenizer.convert_ids_to_tokens(tokenizer(sent, add_special_tokens=False)['input_ids'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dataset_size = len(dataset)\n",
    "# print(f'Dataset size: {dataset_size}')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def print_top_refutes(ngram_size, top_n=30):\n",
    "#     ngram_lmi_dict = get_ngram_label_lmi(dataset, n=ngram_size)\n",
    "#     display(highest_lmi_words('REFUTES', ngram_lmi_dict, n=30))\n",
    "#     return ngram_lmi_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# unigrams = print_top_refutes(1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# bigrams = print_top_refutes(2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# trigrams = print_top_refutes(3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## W&B Results"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 11/02/2021 (9:16)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# results_hans = pd.read_csv(join(project_config.TEMP_DIR, 'wandb_export_2021-02-11T09_16_04.782+02_00.csv'))\n",
    "# len(results_hans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hyperparam_experiment_results({\n",
    "#     'mnli_hclass_lex_config.json': 'LexClass-MNLI [Lex. Overlap]',\n",
    "#     'mnli_hclass_sub_config.json': 'LexClass-MNLI [Subsequence]'\n",
    "# }, results_hans, metric='online_cdl', title='MDL vs. DFL focusing parameter $\\gamma$')\n",
    "# plot_hyperparam_experiment_results({\n",
    "#     'snli_hclass_lex_config.json': 'LexClass-SNLI [Lex. Overlap]',\n",
    "#     'snli_hclass_sub_config.json': 'LexClass-SNLI [Subsequence]'\n",
    "# }, results_hans, metric='online_cdl', title='MDL vs. DFL focusing parameter $\\gamma$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# plot_hyperparam_experiment_results({\n",
    "#     'mnli_hclass_lex_config.json': 'LexClass-MNLI [Lex. Overlap]',\n",
    "#     'mnli_hclass_sub_config.json': 'LexClass-MNLI [Subsequence]'\n",
    "# }, results_hans, \n",
    "#     metric='online_cdl', title=r'MDL vs. PoE weighing parameter $\\alpha$',\n",
    "#     key='alpha', x_label=r'$\\alpha$')\n",
    "# plot_hyperparam_experiment_results({\n",
    "#     'snli_hclass_lex_config.json': 'LexClass-SNLI [Lex. Overlap]',\n",
    "#     'snli_hclass_sub_config.json': 'LexClass-SNLI [Subsequence]'\n",
    "# }, results_hans, \n",
    "#     metric='online_cdl', title=r'MDL vs. PoE weighing parameter $\\alpha$',\n",
    "#     key='alpha', x_label=r'$\\alpha$')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# show_results(results_hans, tasks_hans)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def fever_neg_words_results(task_name, **kwargs):\n",
    "#     task_config_file = f'{task_name}.json'\n",
    "#     header_mapping = {\n",
    "# #         'online_cdl': '$L_{\\mathrm{online}}$',\n",
    "#         'compression': '$\\mathcal{C}$',\n",
    "#         'eval_accuracy_probing': 'Acc.',\n",
    "# #         'eval_fever_symmetric.accuracy_0': 'Symmetric (Supports)',\n",
    "# #         'eval_fever_symmetric.accuracy_1': 'Symmetric (Refutes)',\n",
    "# #         'eval_fever_symmetric.accuracy': 'Symmetric',\n",
    "#         'eval_fever_symmetric_hard.accuracy': 'Hard',\n",
    "#         'eval_fever_symmetric_hard.accuracy_0': 'Hard (Supports)',\n",
    "#         'eval_fever_symmetric_hard.accuracy_1': 'Hard (Refutes)',\n",
    "#     }\n",
    "#     return get_results_table(task_config_file, header_mapping, index_mapping_fever, \n",
    "#                              drop=['tiny'], **kwargs)\n",
    "\n",
    "# def multi_nli_lex_class_results(task_keys: TaskKeys, **kwargs):\n",
    "#     task_name = task_keys.task_name\n",
    "#     task_config_file = f'{task_name}.json'\n",
    "#     header_mapping = {\n",
    "# #         'online_cdl': '$L_{\\mathrm{online}}$',\n",
    "#         'compression': '\\multicolumn{1}{c}{$\\mathcal{C}$}',\n",
    "#         'eval_accuracy_probing': '\\multicolumn{1}{c}{Acc.}',\n",
    "#         task_keys.anti_bias_key: task_keys.anti_bias_label\n",
    "#     }\n",
    "#     results = get_results_table(task_config_file, header_mapping, index_mapping, \n",
    "#                              drop=['tiny', 'hans_poe_32b', 'hans_dfl_32b', \n",
    "#                                    'hans_confreg', 'hypo_confreg', 'confreg', 'hypo_only'], **kwargs)\n",
    "#     return results\n",
    "\n",
    "\n",
    "# def multi_nli_neg_words_results(task_keys: TaskKeys, **kwargs):\n",
    "#     task_name = task_keys.task_name\n",
    "#     task_config_file = f'{task_name}.json'\n",
    "#     header_mapping = {\n",
    "#         'compression': '\\multicolumn{1}{c}{$\\mathcal{C}$}',\n",
    "#         'eval_accuracy_probing': 'Acc.',\n",
    "#         task_keys.anti_bias_key: task_keys.anti_bias_label\n",
    "#     }\n",
    "#     results = get_results_table(task_config_file, header_mapping, index_mapping, \n",
    "#                              drop=['tiny', 'hans_poe_32b', 'hans_dfl_32b', \n",
    "#                                    'hans_confreg', 'hypo_confreg', 'confreg', 'hypo_only'], **kwargs)\n",
    "#     return results\n",
    "\n",
    "# def snli_lex_class_results(task_keys, **kwargs):\n",
    "#     task_name = task_keys.task_name\n",
    "#     task_config_file = f'{task_name}.json'\n",
    "#     header_mapping = {\n",
    "# #         'online_cdl': '$L_{\\mathrm{online}}$',\n",
    "#         'compression': '$\\mathcal{C}$',\n",
    "#         'eval_accuracy_probing': 'Acc.',\n",
    "#         task_keys.anti_bias_key: task_keys.anti_bias_label\n",
    "#     }\n",
    "#     return get_results_table(task_config_file, header_mapping, index_mapping, \n",
    "#                              drop=['tiny', 'hans_confreg', 'confreg'], **kwargs)\n",
    "\n",
    "\n",
    "# def snli_neg_words_results(task_name, **kwargs):\n",
    "#     task_config_file = f'{task_name}.json'\n",
    "#     header_mapping = {\n",
    "#         'compression': '$\\mathcal{C}$',\n",
    "#         'eval_accuracy_probing': 'Acc.',\n",
    "#         'eval_snli_hard.accuracy': 'Hard'\n",
    "#     }\n",
    "#     return get_results_table(task_config_file, header_mapping, index_mapping, \n",
    "#                              drop=['tiny', 'hans_confreg', 'confreg'], **kwargs)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## From JSON"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def drop_columns_starting_with(df, s):\n",
    "    return df.drop(df.columns[df.columns.str.startswith(s)], axis='columns')\n",
    "\n",
    "\n",
    "def parse_json_str(s):\n",
    "    if s is None or s == 'None':\n",
    "        return dict()\n",
    "    return json.loads(s.replace(\"'\", '\"').replace('None', 'null'))\n",
    "\n",
    "\n",
    "def get_row_json_keys(df, name):\n",
    "    return list(parse_json_str(df.iloc[df[name].first_valid_index()][name]).keys())\n",
    "\n",
    "\n",
    "def normalize_columns(col):\n",
    "        if 'accuracy' in col.name:\n",
    "            return col * 100\n",
    "        return col\n",
    "\n",
    "\n",
    "def online_code_results():\n",
    "    df_oc = pd.read_csv(join(project_config.RESULTS_DIR, 'results_online_code.csv'))\\\n",
    "    .drop(['Unnamed: 0', 'cache_only', 'config_dir', 'logging_dir', 'learning_rate',\n",
    "           'task_type', 'batch_size',\n",
    "           'mdl_fractions', 'early_stopping', 'max_seq_length', 'hypothesis_only',\n",
    "          'new_split_ratio', 'overwrite_cache', 'checkpoint_steps', 'min_dataset_size',\n",
    "          'num_train_epochs', 'train_batch_size', 'task_mapper_kwargs', 'wandb_project_name',\n",
    "          'early_stopping_tolerance', 'task_heuristic', 'task_n', 'task_joint', 'task_negative_vocab',\n",
    "          '_step', 'fraction', 'embedding_size', '_timestamp', '_runtime', 'name'], axis='columns', errors='ignore')\\\n",
    "    .rename({'name': 'run_name', 'name.1': 'name', 'model_name_or_path': 'model_name'}, axis='columns')\n",
    "\n",
    "    df_oc['model_name'] = df_oc['model_name'].map(lambda x: re.sub('seed:[0-9]+/', '', x))\n",
    "    df_oc['eval_accuracy'] = df_oc['eval_accuracy'] * 100\n",
    "    df_oc = df_oc.drop('online_cdl', axis='columns')\n",
    "    # df_oc['online_cdl'] = df_oc['online_cdl'] / 1000\n",
    "    df_oc = df_oc.round(2)\n",
    "    df_oc['eval_accuracy'] = df_oc['eval_accuracy'].round(1)\n",
    "    return df_oc\n",
    "\n",
    "\n",
    "def fine_tuning_results():\n",
    "    df = pd.read_csv(join(project_config.RESULTS_DIR, 'results_debiasing.csv'))\n",
    "    df = drop_columns_starting_with(df, 'gradients/')\n",
    "    df = df.drop(['Unnamed: 0', 'fp16', 'debug', 'id2label', \n",
    "                  'label2id', 'top_k', 'top_p', 'adafactor', 'deepspeed',\n",
    "                 'do_sample', 'num_beams', 'report_to', 'use_cache', 'adam_beta1', 'adam_beta2',\n",
    "                 'do_predict', 'hidden_act', 'is_decoder', 'local_rank', 'max_length', 'min_length',\n",
    "                 'model_type', 'past_index', 'save_steps', 'vocab_size', 'hidden_size', 'label_names',\n",
    "                 'logging_dir', 'return_dict', 'sharded_ddp', 'temperature', 'torchscript', 'bos_token_id',\n",
    "                 'disable_tqdm', 'eos_token_id', 'fp16_backend', 'pad_token_id', 'pruned_heads', 'sep_token_id',\n",
    "                 'use_bfloat16', 'bad_words_ids', 'mp_parameters', 'output_scores', 'save_strategy',\n",
    "                 'tpu_num_cores', 'early_stopping', 'fp16_full_eval', 'fp16_opt_level', 'layer_norm_eps',\n",
    "                 'length_penalty', 'finetuning_task', 'group_by_length', 'num_beam_groups', 'overwrite_cache',\n",
    "                 'tokenizer_class', 'type_vocab_size', 'eval_datasets'], axis='columns', errors='ignore')\n",
    "    \n",
    "    df = df.drop(df.columns[df.isnull().all()], axis='columns')\n",
    "\n",
    "    columns = df.columns[df.columns.str.startswith('eval/')]\n",
    "    for col in columns:\n",
    "        if df[col].dtype == object:\n",
    "            keys = get_row_json_keys(df, col)\n",
    "            old_key = col.replace(\"eval_\", \"eval/\")\n",
    "            new_key = col.replace(\"eval/\", \"eval_\")\n",
    "            for key in keys:\n",
    "                def dict_mapper(d):\n",
    "                    if not isinstance(d, str):\n",
    "                        return np.nan\n",
    "                    res = parse_json_str(d)\n",
    "                    if not key in res:\n",
    "                        return np.nan\n",
    "                    return res[key]\n",
    "                if old_key in df.columns:\n",
    "                    df[f'{new_key}.{key}'] = \\\n",
    "                    df[col]\\\n",
    "                    .map(dict_mapper)\\\n",
    "                    .combine_first(df[old_key].map(dict_mapper))\n",
    "                else:\n",
    "                    df[f'{new_key}.{key}'] = \\\n",
    "                    df[col]\\\n",
    "                    .map(dict_mapper)\n",
    "            df = df.drop(col, axis='columns')\n",
    "    \n",
    "    # Legacy fix\n",
    "    columns = df.columns[df.columns.str.startswith('eval_')]\n",
    "    for col in columns:\n",
    "        if df[col].dtype == object:\n",
    "            keys = get_row_json_keys(df, col)\n",
    "            for key in keys:\n",
    "                def dict_mapper(d):\n",
    "                    if not isinstance(d, str):\n",
    "                        return np.nan\n",
    "                    res = parse_json_str(d)\n",
    "                    if not key in res:\n",
    "                        return np.nan\n",
    "                    return res[key]\n",
    "                new_key = f'{col}.{key}'\n",
    "                if new_key in df.columns:\n",
    "                    df[f'{col}.{key}'] = df[f'{col}.{key}'].combine_first(df[col].map(dict_mapper))\n",
    "                else:\n",
    "                    df[f'{col}.{key}'] = df[col].map(dict_mapper)\n",
    "            df = df.drop(col, axis='columns')\n",
    "    \n",
    "    df = df.drop(df.columns[df.isnull().all()], axis='columns')\n",
    "\n",
    "    df = df[['tag', 'seed'] + list(df.columns[df.columns.str.startswith('eval_')])]\\\n",
    "        .drop(['eval_steps', 'eval_batch_size', 'eval_accumulation_steps', 'eval_datasets', 'eval_report'], axis='columns', errors='ignore')\\\n",
    "        .drop(df.columns[df.columns.str.contains('hard_mismatched|eval_report')], axis='columns', errors='ignore')\\\n",
    "        .drop(df.columns[df.columns.str.contains('f1|recall|precision')], axis='columns', errors='ignore')\\\n",
    "        .rename({'tag': 'model_name'}, axis='columns')\\\n",
    "        .apply(normalize_columns)\n",
    "\n",
    "    return df\n",
    "\n",
    "\n",
    "def merge_results(online_code_results, fine_tuning_results):\n",
    "    return pd.merge(fine_tuning_results, \n",
    "                    online_code_results, \n",
    "                    how='outer',\n",
    "                    on=['model_name', 'seed'],\n",
    "                    suffixes=('_downstream', '_probing')\n",
    "                   )\n",
    "    \n",
    "    \n",
    "def results_for_task(df, task_config_file: str, agg=['mean', 'std']):\n",
    "    temp = df\\\n",
    "        .groupby(['task_config_file', 'name'])\\\n",
    "        .agg(agg)\\\n",
    "        .loc[[task_config_file]]\\\n",
    "        .sort_values(('compression', 'mean'))\\\n",
    "        .dropna(how='all', axis='columns')\\\n",
    "        .round(2)\n",
    "    temp = temp.fillna('-')\n",
    "    pred = (temp == 0).all(axis=0)\n",
    "    temp = temp.drop([i for i in pred.index if pred[i]], axis='columns')\n",
    "    return temp\n",
    "\n",
    "\n",
    "def to_latex(results):\n",
    "    print(results.to_latex(\n",
    "        escape=False,\n",
    "        index=True,\n",
    "        column_format='ll' + 'r' * len(results.columns),\n",
    "        multicolumn_format='c'\n",
    "    ))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Finetuning"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_ft = fine_tuning_results()\n",
    "df_ft"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Online Code"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oc = online_code_results()\n",
    "df_oc"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_merged = merge_results(df_oc, df_ft)\n",
    "df_merged"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Camera Ready"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from dataclasses import dataclass\n",
    "\n",
    "annotation_dict = {\n",
    "    'implicit_poe': 'Impl-PoE',\n",
    "    'implicit_dfl': 'Impl-DFL',\n",
    "    'implicit_confreg': 'ConfReg',\n",
    "    'hans_poe': 'HANS-PoE',\n",
    "    'hans_dfl': 'HANS-DFL',\n",
    "    'implicit_poe_e2e': 'Impl-PoE-E2E',\n",
    "    'implicit_dfl_e2e': 'Impl-DFL-E2E',\n",
    "    'implicit_dfl_2k': 'Impl-DFL-Subset',\n",
    "    'hypo_poe': 'Hypo-PoE',\n",
    "    'hypo_dfl': 'Hypo-DFL'\n",
    "    # 'implicit_poe_2k': 'Impl-PoE-Subset',\n",
    "}\n",
    "\n",
    "index_mapping = {\n",
    "    'random': ('', 'Random'),\n",
    "    'pretrained': ('', 'Pretrained'),\n",
    "    'baseline': ('', 'Base'),\n",
    "    'implicit_poe': ('Impl. [TinyBERT]', 'PoE'),\n",
    "    'implicit_dfl': ('Impl. [TinyBERT]', 'DFL'),\n",
    "    'implicit_poe_e2e': ('Impl. [TinyBERT]', 'PoE [E2E]'),\n",
    "    'implicit_dfl_e2e': ('Impl. [TinyBERT]', 'DFL [E2E]'),\n",
    "    'tiny': ('', 'TinyBERT'),\n",
    "    'hans_poe': ('HANS', 'PoE [E2E]'),\n",
    "    'hans_dfl': ('HANS', 'DFL [E2E]'),\n",
    "    'hypo_poe': ('Hypothesis', 'PoE [E2E]'),\n",
    "    'hypo_dfl': ('Hypothesis', 'DFL [E2E]'),\n",
    "    'implicit_confreg': ('Impl. [Subset]', 'ConfReg'),\n",
    "    'implicit_dfl_2k': ('Impl. [Subset]', 'DFL'),\n",
    "    'implicit_poe_2k': ('Impl. [Subset]', 'PoE'),\n",
    "}\n",
    "\n",
    "index_mapping_fever = {\n",
    "    **index_mapping,\n",
    "    'hypo_poe': ('Claim', 'PoE [E2E]'),\n",
    "    'hypo_dfl': ('Claim', 'DFL [E2E]'),\n",
    "}\n",
    "\n",
    "@dataclass\n",
    "class TaskKeys:\n",
    "    task_name: str\n",
    "    anti_bias_key: str\n",
    "    anti_bias_label: str\n",
    "    scatter_title: str = None"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Camera Ready Tables\n",
    "INDEX_NAME = 'Model'\n",
    "\n",
    "def combine_mean_std(values):\n",
    "    if values.str.contains('-').all():\n",
    "        return '-'\n",
    "    elif values.str.contains('-').any():\n",
    "        return f'${values[0]}$'\n",
    "    \n",
    "    return '$' + r' \\pm '.join(values.apply(lambda x: '{0:.2f}'.format(float(x)))) + '$'\n",
    "\n",
    "\n",
    "\n",
    "def get_results_table(task_config_file, header_mapping, index_mapping, drop=[], rename=True, std=True):\n",
    "    if len(header_mapping) == 0:\n",
    "        raise ValueError('header_mapping must contain at least one key to map')\n",
    "    keys = header_mapping.keys()\n",
    "    \n",
    "    results = results_for_task(df_merged, task_config_file)[keys].apply(normalize_columns)\n",
    "    results.columns = results.columns.map('::'.join)\n",
    "    # display(results)\n",
    "    \n",
    "    if std:\n",
    "        results = results\\\n",
    "            .groupby(lambda x: x.split('::')[0], axis=1)\\\n",
    "            .apply(lambda x: x.astype(str).apply(combine_mean_std, 1))[keys]\\\n",
    "    \n",
    "    results = results.drop([(task_config_file, x) for x in drop], errors='ignore')\n",
    "        \n",
    "    if rename:\n",
    "        results = results\\\n",
    "        .rename(header_mapping, axis='columns')\n",
    "    \n",
    "    results = results.loc[task_config_file]\n",
    "    if rename:\n",
    "        results = results.rename({'name': INDEX_NAME}, axis='columns')\n",
    "        results.index = results.index.map(lambda x: index_mapping[x])\n",
    "        results.index.names = ['Bias', INDEX_NAME]\n",
    "        results = results.sort_index()\n",
    "    return results\n",
    "\n",
    "\n",
    "def task_results(task_keys: TaskKeys, **kwargs):\n",
    "    task_name = task_keys.task_name\n",
    "    task_config_file = f'{task_name}.json'\n",
    "    header_mapping = {\n",
    "#         'online_cdl': '$L_{\\mathrm{online}}$',\n",
    "        'compression': '\\multicolumn{1}{c}{$\\mathcal{C}$}',\n",
    "        'eval_accuracy_probing': '\\multicolumn{1}{c}{Acc.}',\n",
    "        task_keys.anti_bias_key: task_keys.anti_bias_label\n",
    "    }\n",
    "    results = get_results_table(task_config_file, header_mapping, index_mapping, \n",
    "                             drop=['tiny', 'hans_poe_32b', 'hans_dfl_32b', \n",
    "                                   'hans_confreg', 'hypo_confreg', 'confreg', 'hypo_only'], **kwargs)\n",
    "    return results\n",
    "\n",
    "\n",
    "# Polynomial Regression\n",
    "def polyfit(x, y, degree):\n",
    "    results = {}\n",
    "\n",
    "    coeffs = np.polyfit(x, y, degree)\n",
    "\n",
    "     # Polynomial Coefficients\n",
    "    results['polynomial'] = coeffs.tolist()\n",
    "\n",
    "    # r-squared\n",
    "    p = np.poly1d(coeffs)\n",
    "    # fit values, and mean\n",
    "    yhat = p(x)                         # or [p(z) for z in x]\n",
    "    ybar = np.sum(y)/len(y)          # or sum(y)/len(y)\n",
    "    ssreg = np.sum((yhat-ybar)**2)   # or sum([ (yihat - ybar)**2 for yihat in yhat])\n",
    "    sstot = np.sum((y - ybar)**2)    # or sum([ (yi - ybar)**2 for yi in y])\n",
    "    results['determination'] = ssreg / sstot\n",
    "\n",
    "    return results\n",
    "\n",
    "\n",
    "def delta_graph(task_keys: TaskKeys, figure_name, return_points=False):\n",
    "    task_name = task_keys.task_name\n",
    "    x_key = task_keys.anti_bias_key + '::mean'\n",
    "    y_key = 'compression::mean'\n",
    "    \n",
    "    temp = task_results(task_keys, rename=False, std=False)\n",
    "    # display(temp)\n",
    "    # temp = temp.set_index(INDEX_NAME)\n",
    "    temp = temp.applymap(lambda x: x if x != '-' else np.nan)\n",
    "    baseline_results = temp.loc['baseline']\n",
    "    temp = temp - baseline_results\n",
    "    temp = temp.drop(['baseline'], axis='index')\n",
    "    \n",
    "    temp = temp.reset_index()\n",
    "    if temp.columns.nlevels > 1:\n",
    "        temp = temp.droplevel(1, axis='columns')\n",
    "        \n",
    "    temp = temp[[c for c in temp.columns if '::std' not in c]]\n",
    "    temp = temp.dropna()\n",
    "    x = temp[x_key].dropna()\n",
    "    y = temp[y_key].dropna()\n",
    "    correlation = x.corr(y)\n",
    "    if return_points:\n",
    "        return (x, y), (baseline_results[x_key], baseline_results[y_key])\n",
    "    \n",
    "    print(f'Pearson = {correlation}\\n')\n",
    "\n",
    "    br = baseline_results\n",
    "    ax = temp.plot.scatter(x=x_key, y=y_key, c='forestgreen', s=80, marker='^', figsize=(4.7747, 3.5))\n",
    "    ax.scatter(0, 0, c='tab:red', s=80, marker='*')\n",
    "    ax.text(0.1, 0.9, r'$\\rho = {:.3f}$'.format(correlation),\n",
    "            horizontalalignment='center',\n",
    "            verticalalignment='center',\n",
    "            transform = ax.transAxes,\n",
    "            backgroundcolor='white')\n",
    "\n",
    "    ax.set_xlabel('$\\Delta\\ \\mathrm{Robustness}$')\n",
    "    ax.set_ylabel('$\\Delta\\ \\mathrm{Bias\\ Extractability}$')\n",
    "#     title = {\n",
    "#         'fever_neg_words': 'FEVER / FEVER-Symmemtric',\n",
    "#         'mnli_lex_class': 'MNLI / HANS',\n",
    "#         'mnli_lex_class_sub': 'MNLI / HANS',\n",
    "#         'mnli_neg_words': 'MNLI / Hard',\n",
    "#         'snli_lex_class': 'SNLI / HANS',\n",
    "#         'snli_lex_class_sub': 'SNLI / HANS',\n",
    "#         'snli_neg_words': 'SNLI / Hard'\n",
    "#     }[task_name]\n",
    "    \n",
    "    pf_results = polyfit(x, y, 1)\n",
    "    print(f'Linear Regression:\\n')\n",
    "    pprint(pf_results)\n",
    "    \n",
    "    m ,b = pf_results['polynomial']\n",
    "    # ax.plot(x, m * x + b, c='burlywood')\n",
    "    \n",
    "    if annotation_dict is not None and False:\n",
    "        temp = temp.set_index('name')\n",
    "        display(temp)\n",
    "        for key, annotation in annotation_dict.items():\n",
    "            if key not in temp.index:\n",
    "                continue\n",
    "            row = temp.loc[key]\n",
    "            point = (row[x_key], row[y_key])\n",
    "            ax.annotate(annotation,\n",
    "                        xy=point,\n",
    "                        xycoords='data',\n",
    "                        xytext=(1.0, 0.0),\n",
    "                        textcoords='offset points')\n",
    "\n",
    "    # LaTeX\n",
    "    try:\n",
    "        plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.pgf'), dpi=300, bbox_inches='tight')\n",
    "    except:\n",
    "        print('Latex Error')\n",
    "    # PNG\n",
    "    plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "    \n",
    "def delta_points(task_keys: TaskKeys):\n",
    "    return delta_graph(task_keys, None, return_points=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### FEVER"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fever_neg_words = TaskKeys(\n",
    "    task_name='fever_neg_words',\n",
    "    anti_bias_key='eval_fever_symmetric_hard.accuracy',\n",
    "    anti_bias_label='Symmetric'\n",
    ")\n",
    "\n",
    "results = task_results(fever_neg_words)\n",
    "display(results)\n",
    "to_latex(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### MNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_lex_class = TaskKeys(\n",
    "    task_name='mnli_lex_class',\n",
    "    anti_bias_key='eval_hans_lexical_overlap.accuracy_1',\n",
    "    anti_bias_label='$\\mathrm{HANS}^-$'\n",
    ")\n",
    "\n",
    "results_mnli_lex_class = task_results(mnli_lex_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_lex_class_sub = TaskKeys(\n",
    "    task_name='mnli_lex_class_sub',\n",
    "    anti_bias_key='eval_hans_subsequence.accuracy_1',\n",
    "    anti_bias_label='$\\mathrm{HANS}^-$'\n",
    ")\n",
    "\n",
    "results_mnli_lex_class_sub = task_results(mnli_lex_class_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results_mnli_lex_class, results_mnli_lex_class_sub], axis=1)\n",
    "display(results)\n",
    "to_latex(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "mnli_neg_words = TaskKeys(\n",
    "    task_name='mnli_neg_words',\n",
    "    anti_bias_key='eval_multi_nli_hard_matched.accuracy',\n",
    "    anti_bias_label='Hard'\n",
    ")\n",
    "\n",
    "results = task_results(mnli_neg_words)\n",
    "display(results)\n",
    "to_latex(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### SNLI"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_lex_class = TaskKeys(\n",
    "    task_name='snli_lex_class',\n",
    "    anti_bias_key='eval_hans_lexical_overlap.accuracy_1',\n",
    "#     anti_bias_key='eval_hans.accuracy_1',\n",
    "    anti_bias_label='$\\mathrm{HANS}^-$'\n",
    ")\n",
    "\n",
    "results_snli_lex_class = task_results(snli_lex_class)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_lex_class_sub = TaskKeys(\n",
    "    task_name='snli_lex_class_sub',\n",
    "    anti_bias_key='eval_hans_subsequence.accuracy_1',\n",
    "#     anti_bias_key='eval_hans.accuracy_1',\n",
    "    anti_bias_label='$\\mathrm{HANS}^-$'\n",
    ")\n",
    "\n",
    "results_snli_lex_class_sub = task_results(snli_lex_class_sub)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = pd.concat([results_snli_lex_class, results_snli_lex_class_sub], axis=1)\n",
    "display(results)\n",
    "to_latex(results)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "snli_neg_words = TaskKeys(\n",
    "    task_name='snli_neg_words',\n",
    "    anti_bias_key='eval_snli_hard.accuracy',\n",
    "    anti_bias_label='Hard'\n",
    ")\n",
    "\n",
    "results = task_results(snli_neg_words)\n",
    "display(results)\n",
    "to_latex(results)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Delta Graphs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "delta_graph(mnli_lex_class, 'scatter_mnli_lex_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(mnli_lex_class_sub, 'scatter_mnli_lex_class_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(mnli_neg_words, 'scatter_mnli_neg_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(fever_neg_words, 'scatter_fever_neg_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(snli_lex_class, 'scatter_snli_lex_class')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(snli_lex_class_sub, 'scatter_snli_lex_class_sub')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "delta_graph(snli_neg_words, 'scatter_snli_neg_words')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "frames = []\n",
    "for task in [\n",
    "    mnli_lex_class,\n",
    "    mnli_lex_class_sub,\n",
    "    snli_lex_class,\n",
    "    snli_lex_class_sub,\n",
    "    fever_neg_words,\n",
    "    mnli_neg_words,\n",
    "    snli_neg_words\n",
    "]:\n",
    "    (x, y), (bx, by) = delta_points(task)\n",
    "    nx = (x).div(bx).rename('x')\n",
    "    ny = (y).div(by).rename('y')\n",
    "\n",
    "    frames.append(pd.concat([nx, ny], axis=1))\n",
    "\n",
    "df = pd.concat(frames, axis=0).reset_index()\n",
    "# display(df)\n",
    "ax = df.plot.scatter(x='x', y='y', c='royalblue', s=120, marker='.')\n",
    "ax.scatter(0, 0, c='tab:red', marker='*', s=120)\n",
    "ax.set_xlabel('$\\Delta\\ \\mathrm{Robustness}\\ [\\%]$')\n",
    "ax.set_ylabel('$\\Delta\\ \\mathrm{Extractability}\\ [\\%]$')\n",
    "figure_name = 'scatter_full'\n",
    "print(f'Pearson = {df[\"x\"].corr(df[\"y\"])}')\n",
    "# LaTeX\n",
    "plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.pgf'), dpi=300, bbox_inches='tight')\n",
    "# PNG\n",
    "plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Retraining"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def retraining_results(dataset_name, prefix, title=None):\n",
    "    df_ft = fine_tuning_results()\n",
    "    retrained_df = df_ft[df_ft.model_name.str.startswith(f'{prefix}/') & df_ft.model_name.str.contains(dataset_name)]\\\n",
    "        .groupby('model_name')\\\n",
    "        .mean()[['eval_hans.accuracy_0', 'eval_hans.accuracy_1']].rename({\n",
    "            'eval_hans.accuracy_0': 'hans-ent-retrained',\n",
    "            'eval_hans.accuracy_1': 'hans-non-ent-retrained'\n",
    "        }, axis='columns')\n",
    "\n",
    "    retrained_df.index = retrained_df.index.map(lambda s: s.replace(f'{prefix}/', ''))\n",
    "    original_models = list(retrained_df.index.values)\n",
    "    original_df = df_ft[df_ft.model_name.isin(original_models)]\\\n",
    "        .groupby('model_name')\\\n",
    "        .mean()[['eval_hans.accuracy_0', 'eval_hans.accuracy_1']].rename({\n",
    "            'eval_hans.accuracy_0': 'hans-ent',\n",
    "            'eval_hans.accuracy_1': 'hans-non-ent'\n",
    "        }, axis='columns')\n",
    "\n",
    "    df = pd.concat([original_df, retrained_df], axis=1)\n",
    "    df.index = df.index.map(lambda s: s.replace(f'{dataset_name}_', '').replace('bert_', '').replace('/', '-').replace('_', '-')).rename('name')\n",
    "    df = df.drop(df.index[df.index.str.contains('hypo')])\n",
    "    ax = df.plot.bar()\n",
    "    if title is not None:\n",
    "        ax.set_title(title)\n",
    "    ax.set_xticklabels(labels=ax.get_xticklabels(), rotation=45, ha='right')\n",
    "    plt.legend(bbox_to_anchor=(1.03, 1), loc='upper left')\n",
    "    y_baseline = df[df.index.str.contains('baseline')].iloc[0]['hans-non-ent-retrained']\n",
    "    plt.axhline(y=y_baseline)\n",
    "\n",
    "    figure_name = f'retrained_{dataset_name}_{prefix}'\n",
    "    return plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.png'), dpi=300, bbox_inches='tight')\n",
    "\n",
    "\n",
    "retraining_results('multi_nli', 'retrain2', 'MNLI')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "retraining_results('multi_nli', 'retrain3', 'MNLI')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Experiment: Varying the Debiasing Effect"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# def plot_hyperparam_experiment_results(task_dict, results: pd.DataFrame, logy=False,\n",
    "#                                        probe_type='linear',\n",
    "#                                        key='dfl_gamma',\n",
    "#                                        title='',\n",
    "#                                        x_label='$\\gamma$',\n",
    "#                                        y_label='MDL',\n",
    "#                                        metric='eval_accuracy'):\n",
    "#     fig0, ax0 = plt.subplots()\n",
    "#     # ax1 = ax0.twinx()\n",
    "    \n",
    "#     for task_name, display_name in task_dict.items():\n",
    "#         df = results_for_task_all_seeds(results, task_name, probe_type=probe_type).sort_index()\n",
    "#         df = df[df.index.str.contains(key)]\n",
    "#         df.index = df.index.map(lambda s: float(s.split('_')[-1]))\n",
    "\n",
    "#         width = 0.2\n",
    "#         df[(metric, 'mean')].plot(kind='line', marker='v',\n",
    "#                                   logy=logy, stacked=True, ax=ax0, label=f'{display_name}', rot=0,\n",
    "#                                   yerr=df[(metric, 'std')])\n",
    "#     #     df[('eval_accuracy', 'mean')].plot(kind='line', color='tab:orange', marker='^', \\\n",
    "#     #                                        logy=logy, secondary_y=True, ax=ax1, label='Acc.', \\\n",
    "#     #                                        yerr=df[('eval_accuracy', 'std')])\n",
    "    \n",
    "#     plt.title(title)\n",
    "#     ax0.set_xlabel(x_label)\n",
    "#     ax0.set_ylabel(y_label)\n",
    "#     # ax1.set_xlabel('$\\gamma$')\n",
    "#     plt.legend()\n",
    "#     ax0.legend()\n",
    "#     # ax0.grid(False)\n",
    "#     # ax1.grid(False)\n",
    "#     base_dir = join(figures_dir, key)\n",
    "#     os.makedirs(base_dir, exist_ok=True)\n",
    "#     filename = re.sub(r'_|(config.json)', ' ', '-'.join(task_dict.keys())) + f'-{key}-{metric}'\n",
    "#     plt.savefig(join(base_dir, filename.replace(' ', '_') + '.png'), dpi=300)\n",
    "#     plt.show()\n",
    "#     plt.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "name_to_title = {\n",
    "    'mnli-lex-class': r'Overlap', \n",
    "    'mnli-lex-class-sub': r'Subsequence'\n",
    "}\n",
    "\n",
    "plt.figure(figsize=(4.7747, 3.5))\n",
    "for task_name in name_to_title.keys():\n",
    "    df = df_merged[df_merged.model_name.str.contains('dfl_gamma') & (df_merged.task_name == task_name)]\\\n",
    "        [['model_name', 'seed', 'compression', 'task_name']]\n",
    "    df['gamma'] = df['model_name'].str.split('_').str[-1].astype(float)\n",
    "    df['model_name'] = df['model_name'].str.split('_').apply(lambda x: '_'.join(x[:-1]))\n",
    "    unstacked = df.groupby(['model_name', 'gamma']).agg(['mean', 'std'])\n",
    "    display(unstacked)\n",
    "    unstacked = unstacked.reset_index().unstack(level=0)\n",
    "\n",
    "    x = unstacked['gamma']\n",
    "    y = unstacked['compression']['mean']\n",
    "    yerr = unstacked['compression']['std']\n",
    "\n",
    "    # plt.plot(x, y)\n",
    "    plt.errorbar(x=x, y=y, yerr=yerr, label=name_to_title[task_name])\n",
    "    plt.xlabel('$\\gamma$')\n",
    "    plt.ylabel('Compression')\n",
    "    plt.legend()\n",
    "\n",
    "figure_name = 'exp_dfl_gamma'\n",
    "# plt.show()\n",
    "plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.pgf'), dpi=300, bbox_inches='tight')\n",
    "plt.savefig(join(project_config.FIGURES_DIR, f'{figure_name}.png'), dpi=300, bbox_inches='tight')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.4"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
